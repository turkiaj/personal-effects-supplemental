---
title: "XGBoost comparison"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra) # for good looking tables

knitr::opts_chunk$set(echo = TRUE, fig.align="center")

# this allows using tikz rendering for plots with "dev=tikz"
knit_hooks$set(plot = function(x, options) {
  if ('tikz' %in% options$dev && !options$external) {
    hook_plot_tex(x, options)
  } else hook_plot_md(x, options)
})

# Fix seed for random number generator for getting consistent results in kmeans etc.
fixed_seed <- 678

# Load common MEBN package
source("mebn/MEBN.r")
```

```{r data_loading, echo=FALSE, message=FALSE}

# Read the data description
datadesc <- read.csv(file="data/SYSDIMET_data_description.csv", header = TRUE, sep = ";")

# Read the actual data matching the description
sysdimet <- read.csv(file="data/SYSDIMET_diet.csv", sep=";", dec=",")

# Define how to iterate through the graph
assumedpredictors <- datadesc[datadesc$Order==100,]    
assumedtargets <- datadesc[datadesc$Order==200,] 
```

## Prediction of future blood test values with XGBoost

For XGBoost, we prepare a dataset that contains all the predictors from past period (nutrients and last response), and also label that tells if the next response goes up or down.

```{r}
diet_input <- sysdimet

# - this is not used, just the field is created
diet_input$prev_fshdl <- diet_input[diet_input$WEEK == 0,]$fshdl
diet_input$prev_fsldl <- diet_input[diet_input$WEEK == 0,]$fsldl
diet_input$prev_fskol <- diet_input[diet_input$WEEK == 0,]$fskol
diet_input$prev_fsins <- diet_input[diet_input$WEEK == 0,]$fsins
diet_input$prev_fpgluk <- diet_input[diet_input$WEEK == 0,]$fpgluk

diet_input[diet_input$WEEK == 4,]$prev_fshdl <- diet_input[diet_input$WEEK == 0,]$fshdl
diet_input[diet_input$WEEK == 8,]$prev_fshdl <- diet_input[diet_input$WEEK == 4,]$fshdl
diet_input[diet_input$WEEK == 12,]$prev_fshdl <- diet_input[diet_input$WEEK == 8,]$fshdl

diet_input[diet_input$WEEK == 4,]$prev_fsldl <- diet_input[diet_input$WEEK == 0,]$fsldl
diet_input[diet_input$WEEK == 8,]$prev_fsldl <- diet_input[diet_input$WEEK == 4,]$fsldl
diet_input[diet_input$WEEK == 12,]$prev_fsldl <- diet_input[diet_input$WEEK == 8,]$fsldl

diet_input[diet_input$WEEK == 4,]$prev_fskol <- diet_input[diet_input$WEEK == 0,]$fskol
diet_input[diet_input$WEEK == 8,]$prev_fskol <- diet_input[diet_input$WEEK == 4,]$fskol
diet_input[diet_input$WEEK == 12,]$prev_fskol <- diet_input[diet_input$WEEK == 8,]$fskol

diet_input[diet_input$WEEK == 4,]$prev_fsins <- diet_input[diet_input$WEEK == 0,]$fsins
diet_input[diet_input$WEEK == 8,]$prev_fsins <- diet_input[diet_input$WEEK == 4,]$fsins
diet_input[diet_input$WEEK == 12,]$prev_fsins <- diet_input[diet_input$WEEK == 8,]$fsins

diet_input[diet_input$WEEK == 4,]$prev_fpgluk <- diet_input[diet_input$WEEK == 0,]$fpgluk
diet_input[diet_input$WEEK == 8,]$prev_fpgluk <- diet_input[diet_input$WEEK == 4,]$fpgluk
diet_input[diet_input$WEEK == 12,]$prev_fpgluk <- diet_input[diet_input$WEEK == 8,]$fpgluk

# remove week 0 from training
#diet_input <- diet_input[diet_input$WEEK >= 4,]

# correct fields for training
input_cols <- list()
input_cols[["fshdl"]] <- c(as.vector(assumedpredictors$Name), "prev_fshdl")
input_cols[["fsldl"]] <- c(as.vector(assumedpredictors$Name), "prev_fsldl")
input_cols[["fskol"]] <- c(as.vector(assumedpredictors$Name), "prev_fskol")
input_cols[["fsins"]] <- c(as.vector(assumedpredictors$Name), "prev_fsins")
input_cols[["fpgluk"]] <- c(as.vector(assumedpredictors$Name), "prev_fpgluk")

# Target 
# 0 - next blood test measurement is same or lower than previous
# 1 - next blood test measurement is higher than previous

diet_target <- sysdimet

# - target is the change from previous measurements
# for every patient, week 12 is replaced by change from week 8 to week 12, etc
diet_target[diet_target$WEEK == 12,]$fshdl <- diet_target[diet_target$WEEK == 12,]$fshdl - diet_target[diet_target$WEEK == 8,]$fshdl
diet_target[diet_target$WEEK == 8,]$fshdl <- diet_target[diet_target$WEEK == 8,]$fshdl - diet_target[diet_target$WEEK == 4,]$fshdl
diet_target[diet_target$WEEK == 4,]$fshdl <- diet_target[diet_target$WEEK == 4,]$fshdl - diet_target[diet_target$WEEK == 0,]$fshdl

diet_target[diet_target$WEEK == 12,]$fsldl <- diet_target[diet_target$WEEK == 12,]$fsldl - diet_target[diet_target$WEEK == 8,]$fsldl
diet_target[diet_target$WEEK == 8,]$fsldl <- diet_target[diet_target$WEEK == 8,]$fsldl - diet_target[diet_target$WEEK == 4,]$fsldl
diet_target[diet_target$WEEK == 4,]$fsldl <- diet_target[diet_target$WEEK == 4,]$fsldl - diet_target[diet_target$WEEK == 0,]$fsldl

diet_target[diet_target$WEEK == 12,]$fskol <- diet_target[diet_target$WEEK == 12,]$fskol - diet_target[diet_target$WEEK == 8,]$fskol
diet_target[diet_target$WEEK == 8,]$fskol <- diet_target[diet_target$WEEK == 8,]$fskol - diet_target[diet_target$WEEK == 4,]$fskol
diet_target[diet_target$WEEK == 4,]$fskol <- diet_target[diet_target$WEEK == 4,]$fskol - diet_target[diet_target$WEEK == 0,]$fskol

diet_target[diet_target$WEEK == 12,]$fsins <- diet_target[diet_target$WEEK == 12,]$fsins - diet_target[diet_target$WEEK == 8,]$fsins
diet_target[diet_target$WEEK == 8,]$fsins <- diet_target[diet_target$WEEK == 8,]$fsins - diet_target[diet_target$WEEK == 4,]$fsins
diet_target[diet_target$WEEK == 4,]$fsins <- diet_target[diet_target$WEEK == 4,]$fsins - diet_target[diet_target$WEEK == 0,]$fsins

diet_target[diet_target$WEEK == 12,]$fpgluk <- diet_target[diet_target$WEEK == 12,]$fpgluk - diet_target[diet_target$WEEK == 8,]$fpgluk
diet_target[diet_target$WEEK == 8,]$fpgluk <- diet_target[diet_target$WEEK == 8,]$fpgluk - diet_target[diet_target$WEEK == 4,]$fpgluk
diet_target[diet_target$WEEK == 4,]$fpgluk <- diet_target[diet_target$WEEK == 4,]$fpgluk - diet_target[diet_target$WEEK == 0,]$fpgluk

# - only the sign of change as label for learning
diet_target$fshdl <- sign(diet_target$fshdl)
diet_target$fsldl <- sign(diet_target$fsldl)
diet_target$fskol <- sign(diet_target$fskol)
diet_target$fsins <- sign(diet_target$fsins)
diet_target$fpgluk <- sign(diet_target$fpgluk)

# remove week 0 from target
#diet_target <- diet_target[diet_target$WEEK >= 4,]

# fix -1 sign to 0 label
diet_target[diet_target$fshdl == -1,]$fshdl <- 0
diet_target[diet_target$fsldl == -1,]$fsldl <- 0
diet_target[diet_target$fskol == -1,]$fskol <- 0
diet_target[diet_target$fsins == -1,]$fsins <- 0
diet_target[diet_target$fpgluk == -1,]$fpgluk <- 0


```

Likewise to the Bayesian network, also now a separate XGBoost model is trained for each blood test. XGBoost algorithm has multiple hyperparameters for specifying the model search. Following script implements search of best hyperparameters for each model. Parameters are searched with cross-validation so that any specific data partitioning to training and test sets don't affect the result.  

```{r}
library(xgboost)

n <- nrow(diet_input)
train.index = sample(n,floor(0.80*n))

# Search best parameters for each model 
modelparams <- list(best_param = list(),
                    best_seednumber = 1234,
                    best_logloss = Inf,
                    best_logloss_index = 0)

allparams <- list(modelparams, modelparams, modelparams, modelparams, modelparams)

t<-1

for (target in assumedtargets$Name)
{
  model_input <- diet_input[input_cols[[target]]]

  train.data = as.matrix(model_input[train.index,])
  train.label = as.matrix(diet_target[train.index,][target])

  # These are not needed in cv
  test.data = as.matrix(model_input[-train.index,])
  test.label = as.matrix(diet_target[-train.index,][target])

  # Transform the two data sets into xgb.Matrix
  xgb.train = xgb.DMatrix(data=train.data,label=train.label)

  modelparams <- allparams[[t]]

  for (iter in 1:1000) {
    
      param <- list(objective = "multi:softprob",
            booster="gbtree",
            eval_metric = "mlogloss",
            num_class = 2,
            max_depth = sample(6:10, 1),
            eta = runif(1, .01, .3),
            gamma = runif(1, 0.0, 0.2), 
            subsample = runif(1, .6, .9),
            colsample_bytree = runif(1, .5, .8), 
            min_child_weight = sample(1:40, 1),
            max_delta_step = sample(1:10, 1)
            )
      
      cv.nround = 1000
      cv.nfold = 10
      
      seed.number = sample.int(10000, 1)[[1]]
      set.seed(seed.number)
      model.cv <- xgb.cv(data=xgb.train, params = param, nthread=4, 
                      nfold=cv.nfold, nrounds=cv.nround,
                      verbose = F, early_stopping_rounds=8, maximize=FALSE)
  
      min_logloss_index <- model.cv$best_iteration
      min_logloss <- model.cv$evaluation_log[min_logloss_index]$test_mlogloss_mean
  
      if (min_logloss < modelparams$best_logloss) {
          modelparams$best_logloss = min_logloss
          modelparams$best_logloss_index = min_logloss_index
          modelparams$best_seednumber = seed.number
          modelparams$best_param = param
      }
  }
  
  allparams[[t]] <- modelparams 
  t <- t + 1
}

```

Models are then trained with found parameters and the accuracy is tested with a separated test set

```{r}
library(xgboost)
eval.mat <- matrix(0,ncol=2,nrow=0)

n <- nrow(diet_input)

# index for parameter list
t <- 1

for (target in assumedtargets$Name)
{
  model_input <- diet_input[input_cols[[target]]]
  
  train.data = as.matrix(model_input[train.index,])
  train.label = as.matrix(diet_target[train.index,][target])
  
  test.data = as.matrix(model_input[-train.index,])
  test.label = as.matrix(diet_target[-train.index,][target])
  
  # Transform the two data sets into xgb.Matrix
  xgb.train = xgb.DMatrix(data=train.data,label=train.label)
  xgb.test = xgb.DMatrix(data=test.data,label=test.label)

  # get best parameters for this model
  modelparams <- allparams[[t]]
  set.seed(modelparams$best_seednumber)
  
  model.xgb <- xgb.train(data=xgb.train, 
                  params=modelparams$best_param, 
                  nrounds=modelparams$best_logloss_index, 
                  nthread=4)

  # Evaluate predictions of this model

  xgb.test = xgb.DMatrix(data=test.data,label=test.label)
    
  xgb.pred = predict(model.xgb,test.data,reshape=T)
  xgb.pred = as.data.frame(xgb.pred)
  colnames(xgb.pred) = c(0,1)
    
  # Use the predicted label with the highest probability
  xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
  xgb.pred$label = test.label
  
  # Calculate the final accuracy
  accuracy = sum(as.integer(xgb.pred$prediction==xgb.pred$label))/nrow(xgb.pred)  

  # Cumulate result matrix
  eval.mat <- rbind(eval.mat, c(target, accuracy))

  # Plot feature importaces
  importance_matrix <- xgb.importance(model = model.xgb)
  #print(importance_matrix) 
  xgb.plot.importance(importance_matrix = importance_matrix)
  
  t <- t + 1
}  

saveRDS(eval.mat, "evaluations/xgboost-cv-mat.rds")
eval.mat
```

Without cross-validation

```{r}
library(xgboost)
eval.mat <- matrix(0,ncol=2,nrow=0)

n <- nrow(diet_input)

for (target in assumedtargets$Name)
{
  model_input <- diet_input[input_cols[[target]]]
  
  train.index = sample(n,floor(0.80*n))
  train.data = as.matrix(model_input[train.index,])
  train.label = as.matrix(diet_target[train.index,][target])
  
  test.data = as.matrix(model_input[-train.index,])
  test.label = as.matrix(diet_target[-train.index,][target])
  
  # Transform the two data sets into xgb.Matrix
  xgb.train = xgb.DMatrix(data=train.data,label=train.label)
  xgb.test = xgb.DMatrix(data=test.data,label=test.label)

  num_class = 2
  params = list(
    booster="gbtree",
    eta=0.001,
    max_depth=5,
    gamma=3,
    subsample=0.75,
    colsample_bytree=1,
    objective="multi:softprob",
    eval_metric="mlogloss",
    num_class=num_class
  )
 
  model.xgb = xgb.train(
    params=params,
    data=xgb.train,
    nrounds=10000,
    nthreads=1,
    early_stopping_rounds=10,
    watchlist=list(val1=xgb.train,val2=xgb.test),
    verbose=0
  )

  xgb.test = xgb.DMatrix(data=test.data,label=test.label)
    
  xgb.pred = predict(model.xgb,test.data,reshape=T)
  xgb.pred = as.data.frame(xgb.pred)
  colnames(xgb.pred) = c(0,1)
    
  # Use the predicted label with the highest probability
  xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
  xgb.pred$label = test.label
  
  # Calculate the final accuracy
  accuracy = sum(as.integer(xgb.pred$prediction==xgb.pred$label))/nrow(xgb.pred)  

  # Cumulate result matrix
  eval.mat <- rbind(eval.mat, c(target, accuracy))
  
  # Plot feature importaces
  importance_matrix <- xgb.importance(model = model.xgb)
  #print(importance_matrix) 
  xgb.plot.importance(importance_matrix = importance_matrix)
}  

saveRDS(eval.mat, "evaluations/xgboost-nocv-mat.rds")
eval.mat
```

